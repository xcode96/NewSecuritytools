// Script to import JSON data into Supabase
import * as fs from 'fs';
import * as path from 'path';
import { createClient } from '@supabase/supabase-js';

// Load environment variables
process.loadEnvFile();
const supabaseUrl = process.env.VITE_SUPABASE_URL || '';
const supabaseKey = process.env.VITE_SUPABASE_ANON_KEY || '';

const supabase = createClient(supabaseUrl, supabaseKey);

const DATA_DIR = './data-export';

// Helper to read JSON
function readJSON(filename: string) {
    const filePath = path.join(DATA_DIR, filename);
    if (!fs.existsSync(filePath)) {
        console.log(`‚ö†Ô∏è  File not found: ${filename}`);
        return [];
    }
    return JSON.parse(fs.readFileSync(filePath, 'utf-8'));
}

// Map Category Names to IDs
let categoryIdMap: Record<string, string> = {};

async function importCategories() {
    console.log('üì• Importing Categories...');

    // 1. Get unique categories from Tools data (source of truth for categories used) plus fixed list?
    // Actually we should import from a categories source or just infer them. 
    // The current exportData.ts DOES NOT export categories explicitly, only tools. 
    // We should extract unique categories from tools.json.
    const tools = readJSON('tools.json');
    const uniqueCategories = Array.from(new Set(tools.map((t: any) => t.category))).filter(Boolean);

    console.log(`   Found ${uniqueCategories.length} unique categories.`);

    for (const catName of uniqueCategories) {
        // Upsert categories
        const { data, error } = await supabase
            .from('categories')
            .upsert({ name: catName }, { onConflict: 'name' })
            .select('id, name')
            .single();

        if (error) {
            console.error(`   ‚ùå Error importing category ${catName}:`, error.message);
        } else if (data) {
            categoryIdMap[data.name] = data.id;
        }
    }
    console.log('‚úÖ Categories imported.\n');
}

async function importTools() {
    console.log('üì• Importing Tools...');

    // Clear existing tools to avoid duplicates (since we don't have unique IDs from local)
    // CAUTION: This wipes the table.
    const { error: deleteError } = await supabase.from('tools').delete().neq('id', '00000000-0000-0000-0000-000000000000');
    if (deleteError) console.error('Warning clearing tools:', deleteError.message);

    const tools = readJSON('tools.json');
    const toolsToInsert = tools.map((t: any) => ({
        name: t.name,
        description: t.description,
        category_id: categoryIdMap[t.category], // Link!
        url: t.url,
        color: t.color,
        icon: t.icon,
        command: t.command,
        tags: t.tags,
        is_hidden: t.isHidden, // map camelCase
        articles: t.articles
    }));

    // Batch insert to avoid payload limits
    const batchSize = 100;
    for (let i = 0; i < toolsToInsert.length; i += batchSize) {
        const batch = toolsToInsert.slice(i, i + batchSize);
        const { error } = await supabase.from('tools').insert(batch);
        if (error) console.error(`‚ùå Error importing tools batch ${i}:`, error.message);
    }
    console.log(`‚úÖ Imported ${tools.length} tools.\n`);
}

async function importGeneric(filename: string, tableName: string) {
    const data = readJSON(filename);
    if (data.length === 0) return;

    console.log(`üì• Importing ${tableName} (${data.length} items)...`);

    // Clear table first
    // Note: dependent tables need to be cleared in order if any.
    // 'categories' is top level. 'tools' depends on 'categories'.
    // Others are independent.
    // We can just delete.
    // However, for tables with bigints/generated IDs, we can't easily query 'id != 0' unless we know the type.
    // Supabase delete requires a filter. .neq('id', 0) works for int ids.
    // For UUIDs, neq '0000...' works or similar.
    // Let's try .neq('id', 0) for generic tables (they use bigint generated by identity).

    const { error: deleteError } = await supabase.from(tableName).delete().neq('id', 0);
    if (deleteError) console.error(`Warning clearing ${tableName}:`, deleteError.message);


    // Batch insert
    const batchSize = 50;
    for (let i = 0; i < data.length; i += batchSize) {
        const batch = data.slice(i, i + batchSize);

        const cleanBatch = batch.map(({ id, ...rest }: any) => {
            // Handle case conversions if needed
            if (tableName === 'certifications') {
                return {
                    ...rest,
                    job_market: rest.jobMarket,
                    exam_format: rest.examFormat,
                    key_skills: rest.keySkills,
                    career_paths: rest.careerPaths
                };
            }
            return rest;
        });

        const { error } = await supabase.from(tableName).insert(cleanBatch);
        if (error) console.error(`   ‚ùå Batch error:`, error.message);
    }
    console.log(`‚úÖ Completed ${tableName}.\n`);
}

async function importAll() {
    console.log('üöÄ Starting Supabase Migration...\n');

    await importCategories();
    await importTools();

    await importGeneric('books.json', 'books');
    await importGeneric('useful-links.json', 'useful_links');
    await importGeneric('platforms.json', 'platforms');
    await importGeneric('certifications.json', 'certifications');
    await importGeneric('downloads.json', 'downloads');
    await importGeneric('frameworks.json', 'frameworks');
    await importGeneric('breach-services.json', 'breach_services');
    await importGeneric('youtubers.json', 'youtubers');

    console.log('‚ú® Migration complete!');
}

importAll().catch(console.error);
